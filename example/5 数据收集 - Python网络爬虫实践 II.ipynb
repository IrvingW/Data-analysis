{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1, align=center> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;数据科学引论 - Python之道 </h1>\n",
    "\n",
    "<h1, align=center> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;第5课 数据收集 - Python网络爬虫实践 II </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 概述\n",
    "接下来，我们通过一个更加复杂但是贴近实际的爬虫例子讲述。\n",
    "\n",
    "这次爬取的内容是 IT桔子 中的新公司成立内容。也就是网站http://www.itjuzi.com/company?sortby=foundtime&page=1  的前10页。\n",
    "\n",
    "爬取内容包括公司名、公司类别成立时间、省份、最新融资情况。最终以csv格式保存到文件。\n",
    "\n",
    "> csv格式是一个常见的存储表格数据的格式，爬虫完成之后的csv文件，可以用excel直接打开。\n",
    "\n",
    "## 注意\n",
    "这个爬虫比之前的样例更加复杂，因为实际的网站中，可能在解析之后要通过一些字符串操作才能得到有效信息，如网页中常出现一些空格和换行来达到良好的显示效果，但是我们爬取的时候是要将这些字符去除。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scrapy\n",
    "import time\n",
    "import csv\n",
    "import os\n",
    "\n",
    "class MySpider(scrapy.Spider):\n",
    "    \n",
    "    name = \"spider\"\n",
    "\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.file = open('demo2_newCompanies.csv', 'w',encoding='utf8',newline='');\n",
    "        self.csvWriter = csv.DictWriter(self.file, fieldnames=['name','type','date','province','status'])\n",
    "        \n",
    "        #设置待爬取网站列表\n",
    "        self.urls = []\n",
    "        for i in range(1,10):\n",
    "            self.urls.append('http://www.itjuzi.com/company?sortby=foundtime&page=' + str(i) )\n",
    "        print(self.urls)\n",
    "        \n",
    "        \n",
    "    def start_requests(self):\n",
    "        #self.init_urls()\n",
    "        for url in self.urls:\n",
    "            yield scrapy.Request(url=url, callback=self.parse)    \n",
    "    \n",
    "\n",
    "    #parse方法会在每个request收到response之后调用\n",
    "    def parse(self, response):\n",
    "        \n",
    "        #提取公司列表\n",
    "        companys = response.css(\".list-main-icnset.list-main-com li\");\n",
    "        #从一开始是为了跳过网页内的表格标题栏\n",
    "        for company in companys:\n",
    "            # 解析公司名\n",
    "            name = company.css(\".title span::text\").extract_first();\n",
    "            # 跳过没有公司名的公司\n",
    "            if name is None:\n",
    "                continue;\n",
    "                   \n",
    "            # 解析公司大类和子类\n",
    "            type = company.css(\".t-small a::text\").extract()\n",
    "            # 解析得到时间\n",
    "            date = company.css(\".date::text\").extract_first()\n",
    "            #去除网页原有的空格、换行、制表符\n",
    "            date = date.replace('\\t','').replace('\\n','').replace(' ','')\n",
    "            # 解析省份\n",
    "            province = company.css(\".addr a::text\").extract_first()\n",
    "            province = province.replace('\\t','').replace('\\n','').replace(' ','')\n",
    "            # 解析最新融资情况\n",
    "            status = company.css(\".status .marl10::text\").extract()\n",
    "            \n",
    "            # 构建字典\n",
    "            item = {\"name\":name, \"type\":type, \"date\":date, \"province\":province, \"status\":status }\n",
    "\n",
    "            # 以csv格式写入文件\n",
    "            self.csvWriter.writerow(item)\n",
    "\n",
    "            \n",
    "            \n",
    "        #及时将内容写入文件，否则可能会出现少许延迟\n",
    "        self.file.flush()\n",
    "        os.fsync(self.file)\n",
    "        #输出当前解析完成的网页网址，可以当做爬取进度来看待,与程序逻辑无关\n",
    "        print(\"over: \" + response.url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-07-31 22:34:45 [scrapy.utils.log] INFO: Scrapy 1.3.3 started (bot: scrapybot)\n",
      "2017-07-31 22:34:45 [scrapy.utils.log] INFO: Overridden settings: {'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}\n",
      "2017-07-31 22:34:45 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.logstats.LogStats']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['http://www.itjuzi.com/company?sortby=foundtime&page=1', 'http://www.itjuzi.com/company?sortby=foundtime&page=2', 'http://www.itjuzi.com/company?sortby=foundtime&page=3', 'http://www.itjuzi.com/company?sortby=foundtime&page=4', 'http://www.itjuzi.com/company?sortby=foundtime&page=5', 'http://www.itjuzi.com/company?sortby=foundtime&page=6', 'http://www.itjuzi.com/company?sortby=foundtime&page=7', 'http://www.itjuzi.com/company?sortby=foundtime&page=8', 'http://www.itjuzi.com/company?sortby=foundtime&page=9']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-07-31 22:34:46 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2017-07-31 22:34:46 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2017-07-31 22:34:46 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2017-07-31 22:34:46 [scrapy.core.engine] INFO: Spider opened\n",
      "2017-07-31 22:34:46 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2017-07-31 22:34:46 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023\n",
      "2017-07-31 22:34:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.itjuzi.com/company?sortby=foundtime&page=4> (referer: None)\n",
      "2017-07-31 22:34:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.itjuzi.com/company?sortby=foundtime&page=7> (referer: None)\n",
      "2017-07-31 22:34:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.itjuzi.com/company?sortby=foundtime&page=2> (referer: None)\n",
      "2017-07-31 22:34:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.itjuzi.com/company?sortby=foundtime&page=1> (referer: None)\n",
      "2017-07-31 22:34:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.itjuzi.com/company?sortby=foundtime&page=8> (referer: None)\n",
      "2017-07-31 22:34:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.itjuzi.com/company?sortby=foundtime&page=5> (referer: None)\n",
      "2017-07-31 22:34:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.itjuzi.com/company?sortby=foundtime&page=3> (referer: None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "over: http://www.itjuzi.com/company?sortby=foundtime&page=4\n",
      "over: http://www.itjuzi.com/company?sortby=foundtime&page=7\n",
      "over: http://www.itjuzi.com/company?sortby=foundtime&page=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-07-31 22:34:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.itjuzi.com/company?sortby=foundtime&page=6> (referer: None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "over: http://www.itjuzi.com/company?sortby=foundtime&page=1\n",
      "over: http://www.itjuzi.com/company?sortby=foundtime&page=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-07-31 22:34:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.itjuzi.com/company?sortby=foundtime&page=9> (referer: None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "over: http://www.itjuzi.com/company?sortby=foundtime&page=5\n",
      "over: http://www.itjuzi.com/company?sortby=foundtime&page=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-07-31 22:34:48 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2017-07-31 22:34:48 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 2340,\n",
      " 'downloader/request_count': 9,\n",
      " 'downloader/request_method_count/GET': 9,\n",
      " 'downloader/response_bytes': 91642,\n",
      " 'downloader/response_count': 9,\n",
      " 'downloader/response_status_count/200': 9,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2017, 7, 31, 14, 34, 48, 89020),\n",
      " 'log_count/DEBUG': 10,\n",
      " 'log_count/INFO': 7,\n",
      " 'response_received_count': 9,\n",
      " 'scheduler/dequeued': 9,\n",
      " 'scheduler/dequeued/memory': 9,\n",
      " 'scheduler/enqueued': 9,\n",
      " 'scheduler/enqueued/memory': 9,\n",
      " 'start_time': datetime.datetime(2017, 7, 31, 14, 34, 46, 282456)}\n",
      "2017-07-31 22:34:48 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "over: http://www.itjuzi.com/company?sortby=foundtime&page=6\n",
      "over: http://www.itjuzi.com/company?sortby=foundtime&page=9\n"
     ]
    }
   ],
   "source": [
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "process = CrawlerProcess({\n",
    "    'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'\n",
    "})\n",
    "\n",
    "process.crawl(MySpider)\n",
    "process.start() # 这句代码就是开始了整个爬虫过程 "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
